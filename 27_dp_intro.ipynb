{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Foundations of Computational Economics #27\n",
    "\n",
    "by Fedor Iskhakov, ANU\n",
    "\n",
    "<img src=\"_static/img/dag3logo.png\" style=\"width:256px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Dynamic programming in discrete world\n",
    "\n",
    "<img src=\"_static/img/lecture.png\" style=\"width:64px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"_static/img/youtube.png\" style=\"width:65px;\">\n",
    "\n",
    "[https://youtu.be/kpNGDQnDpmU](https://youtu.be/kpNGDQnDpmU)\n",
    "\n",
    "Description: Backwards induction. Tiling problem. Deal or no deal game. Bellman optimality principle. Inventory dynamics model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is dynamic programming?\n",
    "\n",
    "**‚ÄúDP is recursive method for solving sequential decision problems‚Äù**\n",
    "\n",
    "üìñ Rust 2006, *New Palgrave Dictionary of Economics*\n",
    "\n",
    "In computer science the meaning of the term is broader:\n",
    "**DP is a general algorithm design technique for solving problems with\n",
    "overlapping sub-problems.**\n",
    "\n",
    "Generally allows solving in polynomial time $ O(n^k) $ problems that would\n",
    "otherwise take exponential time  $ O(a^n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tiling with dominoes example\n",
    "\n",
    "Given a $ 3 \\times n $ board, find **the number of ways** to\n",
    "fill it with $ 2 \\times 1 $ dominoes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Examples of tiling\n",
    "\n",
    "These are three possible ways to fill up $ 3 \\times 2 $ board\n",
    "\n",
    "<img src=\"_static/img/tile1.jpg\" style=\"height:100px;\">\n",
    "\n",
    "This is one possible way to tile $ 3 \\times 8 $ board\n",
    "\n",
    "<img src=\"_static/img/tile2.jpg\" style=\"height:100px;\">\n",
    "\n",
    "The problem is to compute the number of possible tilings for any $ 3 \\times n $ board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Breaking the big problem into subproblems\n",
    "\n",
    "Observe that at any possible stage of filling up a $ 3 \\times n $ board we may have\n",
    "the following configurations\n",
    "\n",
    "<img src=\"_static/img/tile3.jpg\" style=\"height:180px;\">\n",
    "\n",
    "And it is impossible to have the following configurations\n",
    "\n",
    "<img src=\"_static/img/tile4.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Defining the recursion\n",
    "\n",
    "The case of $ A_n $:\n",
    "\n",
    "<img src=\"_static/img/tile5.jpg\" style=\"height:120px;\">\n",
    "\n",
    "The case of $ B_n $:\n",
    "\n",
    "<img src=\"_static/img/tile6.jpg\" style=\"height:120px;\">\n",
    "\n",
    "The case of $ C_n $ is identical to $ B_n $, i.e. $ C_n = B_n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Recursive solution\n",
    "\n",
    "Therefore for any $ n $ we have\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "A_n &=& A_{n-2} + 2 B_{n-1} \\\\\n",
    "B_n &=& A_{n-1} + B_{n-2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "The answer to the whole problem is given by $ A_n $\n",
    "\n",
    "1. Inductive computation of the two sequences.  \n",
    "1. Can be improved by *memoization* (optimization technique based on caching previous calls to the function)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def WaysTileDominoes(n):\n",
    "    '''Compute the number of ways to tile 3 x n area by 2x1 tiles'''\n",
    "    A, B = [0] * (n + 1),[0] * (n + 1)\n",
    "    A[0] = 1 # one way to tile 3x0\n",
    "    A[1] = 0 # no way to tile 3x1\n",
    "    B[0] = 0 # no way to tile 3x0 without a corner\n",
    "    B[1] = 1 # one way to tile 3x1 without a corner\n",
    "    for i in range(2, n+1): # loop over 2,3,..,n\n",
    "        A[i] = A[i-2] + 2 * B[i-1]\n",
    "        B[i] = A[i-1] + B[i-2]\n",
    "    return A[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for n in range(1,20):\n",
    "    print('There are',WaysTileDominoes(n),'ways to tile the 3 by',n,'board')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deal or no deal example\n",
    "\n",
    "Consider a version of [Deal or no deal](https://en.wikipedia.org/wiki/Deal_or_No_Deal) TV show game\n",
    "\n",
    "- the player is presented with $ n $ boxes with prizes hidden inside  \n",
    "- all prizes $ x_1,\\dots,x_n $ are known to the player, but not where they are  \n",
    "- at each round the player may choose a box at random to be removed from the game (and loose the opportunity to get the prize within)  \n",
    "- otherwise, the player may choose to stop the game and walk away with the prize chosen randomly from remaining boxes  \n",
    "\n",
    "\n",
    "What is the optimal strategy to maximize the (expected) reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Need assumption about the reward evaluation\n",
    "\n",
    "- assume that the player is maximizing the expected reward  \n",
    "- ok, but then for each set of remaining prizes $ (x,y,z) $ the expected reward is  \n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{x+y+z}{3} \\text{  (stopping the game)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{3}\\cdot\\frac{x+y}{2} + \\frac{1}{3}\\cdot\\frac{x+z}{2} + \\frac{1}{3}\\cdot\\frac{y+z}{2} \\text{  (removing random box)}\n",
    "$$\n",
    "\n",
    "- at each point of the game the player will be indifferent between stopping or continuing the game  \n",
    "- thus, optimal strategy is to stop at the first round and take random reward!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Loss aversion utility\n",
    "\n",
    "- assume that the player has loss aversion utility of the form  \n",
    "\n",
    "\n",
    "$$\n",
    "U(\\text{reward},\\text{reference}) = \\text{reward} - \\theta \\cdot \\mathbb{1}\\{ \\text{reward}< \\text{reference}\\}\n",
    "$$\n",
    "\n",
    "- if award is below the reference level, there is a cut in utility  \n",
    "- assume further that the reference level is *updated* endogenously during the game:  \n",
    "- at each round equal to the foregone option of stopping the game in the previous round  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How to solve for the optimal strategy in this game?\n",
    "\n",
    "1. What is the maximum number of rounds in the game?  \n",
    "1. What does the optimal choice depend on in each round?  \n",
    "1. What is the *complete* strategy in the game?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- for a given round let $ B $ denote the set of remaining $ n $ boxes/prizes  \n",
    "- denote $ r $ the reference level in the utility function $ U(\\cdot,r) $  \n",
    "\n",
    "\n",
    "Let $ V(B,r) $ be the maximum expected reward, i.e. assuming optimal strategy is played from this round on\n",
    "\n",
    "$$\n",
    "V(B,r) = \\max\\Big[ \\underbrace{\\tfrac{1}{n} \\sum_{b \\in B} U(b,r)}_{\\text{stop}} ;\n",
    "                   \\underbrace{\\sum_{b \\in B} \\tfrac{1}{n} V\\big(B \\backslash b, \\tfrac{1}{n} \\sum_{d \\in B} U(d,r)\\big)  }_{\\text{continue}}\n",
    "                 \\Big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "V(B,r) = \\max\\big[ R_r ; \\sum_{b \\in B} \\tfrac{1}{n} V(B \\backslash b, R_r) \\big]\n",
    "$$\n",
    "\n",
    "- $ R_r = \\tfrac{1}{n} \\sum_{b \\in B} U(b,r) $ is current round expected reward  \n",
    "- $ B \\backslash b $ is the next round set of $ n-1 $ boxes/prizes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def expected_reward(boxes,ref=None,loss_aversion_param=.1,verbose=True):\n",
    "    '''Compute the expected reward from the game with given boxed prizes and the reference level'''\n",
    "    n = len(boxes)  # number of boxed remaining\n",
    "    ref = sum(boxes)/n if ref is None else ref  # default reference level\n",
    "    # reward if stopping\n",
    "    current = [b - loss_aversion_param*(b<ref) for b in boxes]\n",
    "    current = sum(current)/n  # average\n",
    "    # expected reward if continuing the game\n",
    "    if n==1:\n",
    "        next = current  # the same if only one box left\n",
    "    else:\n",
    "        next = 0.  # initialize\n",
    "        for i in range(n):  # accumulate expected reward component by component\n",
    "            next += expected_reward(boxes[:i]+boxes[i+1:],current,loss_aversion_param,verbose)\n",
    "        next /= n  # take average\n",
    "    if verbose:\n",
    "        print('%-20s'%''.join('[{}]'.format(b) for b in boxes),end='')  # this round boxes\n",
    "        print(' ref={:<6.3f}'.format(ref),end='')                       # this round reference level\n",
    "        if n>1:\n",
    "            print(' reward if stop={:<6.3f} if continue={:<6.3f}'.format(current,next),end='')  # rewards for two decisions\n",
    "            print('  >> {}!'.format('stop' if current >= next else 'continue'))                 # best decision\n",
    "        else:\n",
    "            print(' reward={:<6.3f}'.format(current))   # reward in case of last box left\n",
    "    return max(current,next)  # reward from the optimal choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "boxes = [int(math.exp(b)) for b in range(3)]  # uneven prizes\n",
    "print('Initial prizes = ',boxes)\n",
    "expected_reward(boxes,loss_aversion_param=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bellman‚Äôs Principle of Optimality\n",
    "\n",
    "‚ÄúAn optimal policy has a property that whatever the initial state and\n",
    "initial decision are, the remaining decisions must constitute an\n",
    "optimal policy with regard to the state resulting from the first\n",
    "decision.‚Äù\n",
    "\n",
    "üìñ Bellman, 1957 ‚ÄúDynamic Programming‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Breaking the problem into sequence of small problems\n",
    "\n",
    "- Thus, the sequential decision problem is broken into *initial decision*\n",
    "  problem and the *future decisions* problem  \n",
    "- The solution can be computed through **backward induction**,\n",
    "  i.e.¬†solving a sequential decision problem from the later periods  \n",
    "- Embodiment of the recursive way of modeling sequential decisions is\n",
    "  **Bellman equation**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bellman equation\n",
    "\n",
    "$$\n",
    "V(\\text{state}) = \\max_{\\text{decisions}} \\big[ U(\\text{state},\\text{decision}) + \\beta \\mathbb{E}\\big\\{ V(\\text{next state})  \\big| \\text{state},\\text{decision} \\big\\} \\big]\n",
    "$$\n",
    "\n",
    "- $ V(\\text{state}) $ is **value function** = maximum attainable (discounted) expected reward/utility/payoff  \n",
    "- $ U(\\text{state},\\text{decision}) $ is per-period/flow/instantaneous reward/utility/payoff  \n",
    "- (*next state*) is the *stochastic* next period state resulting from current state and decision  \n",
    "- expectation $ \\mathbb{E}\\{\\cdot\\} $ is taken over the distribution of the next period state conditional on current state and decision  \n",
    "- $ \\beta $ is a discount factor to measure future rewards in terms of current ones  \n",
    "\n",
    "\n",
    "The optimal choices are revealed along the solution of the Bellman equation as decisions which solve the maximization problem in the right hand side!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bellman equation in deterministic models\n",
    "\n",
    "In deterministic case, expectation is not necessary:\n",
    "\n",
    "$$\n",
    "V(\\text{state}) = \\max_{\\text{decisions}} \\big[ U(\\text{state},\\text{decision}) + \\beta \\cdot V(\\text{next state}) \\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bellman equation in models with finite horizon\n",
    "\n",
    "Additional condition at the final period $ t=T $, usually\n",
    "\n",
    "$$\n",
    "V(\\text{state}) = \\max_{\\text{decisions}} \\big[ U(\\text{state},\\text{decision}) \\big] \\text{ at terminal period } T\n",
    "$$\n",
    "\n",
    "In other words, as if $ V(\\text{at } T +1 ) = \\mathbb{0} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Power of dynamic programming\n",
    "\n",
    "DP is a the main tool in analyzing modern micro and macto economic models\n",
    "\n",
    "DP is powerful due to its **flexibility and breadth**\n",
    "\n",
    "DP provides a framework to study decision making over time and under uncertainty\n",
    "and can accommodate learning, strategic interactions between agents (game theory)\n",
    "and market interactions (equilibrium theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dynamic programming in economics\n",
    "\n",
    "Many important problems and economic models are analyzed and solved\n",
    "using dynamic programming:\n",
    "\n",
    "- Dynamic models of labor supply  \n",
    "- Job search  \n",
    "- Human capital accumulation  \n",
    "- Health process, insurance and long term care  \n",
    "- Consumption/savings choices  \n",
    "- Durable consumption  \n",
    "- Growth models  \n",
    "- Heterogeneous agents models  \n",
    "- Overlapping generation models  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Origin of the term Dynamic Programming\n",
    "\n",
    "üìñ Bellman‚Äôs autobiography ‚ÄúThe Eye of the Hurricane‚Äù\n",
    "\n",
    "The 1950‚Äôs were not good years for mathematical research. We had a very interesting\n",
    "gentleman in Washington named Wilson. He was Secretary of Defence, and\n",
    "he actually had a pathological fear and hatred of the word ‚Äúresearch‚Äù.\n",
    "\n",
    "I‚Äôm not using the term lightly; I‚Äôm using it precisely. His face would\n",
    "suffuse, he would turn red, and he would get violent if people used the\n",
    "term, research, in his presence. You can imagine how he felt, then,\n",
    "about the term, mathematical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, I felt I had to do something to shield Wilson and the Air Force\n",
    "from the fact that I was really doing mathematics inside the RAND Corporation.\n",
    "\n",
    "What title, what name, could I choose?\n",
    "\n",
    "In the first place, I was interested in planning, in\n",
    "decision-making, in thinking. But planning, is not a good word for\n",
    "various reasons. I decided therefore to use the word, ‚Äúprogramming‚Äù.\n",
    "\n",
    "I wanted to get across the idea that this was dynamic, this was\n",
    "multistage, this was time-varying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I thought, let‚Äôs kill two birds with one stone. Let‚Äôs take a word which has an absolutely precise\n",
    "meaning, namely dynamic, in the classical physical sense.\n",
    "\n",
    "It also has a very interesting property as an adjective, and that is it‚Äôs impossible\n",
    "to use the word, dynamic, in the pejorative sense.\n",
    "\n",
    "Thus, I thought dynamic programming was a good name. It was something not even a\n",
    "Congressman could object to. So I used it as an umbrella for my activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inventory dynamics problem\n",
    "\n",
    "Consider the following problem in discrete time and finite horizon $ t=0,\\dots,T $\n",
    "\n",
    "The notation is:\n",
    "\n",
    "- $ x_t\\ge 0 $ is inventory at period $ t $ measured in **discrete units**  \n",
    "- $ d_t\\ge 0 $ is *potentially stochastic* demand at period $ t $  \n",
    "- $ q_t\\ge 0 $ is the order of new inventory  \n",
    "- $ p $ is the profit per one unit of (supplied) good  \n",
    "- $ c $ is the fixed cost of ordering any amount of new inventory  \n",
    "- $ r $ is the cost of storing one unit of good  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The sales in period $ t $ are given by $ s_t = \\min\\{x_t,d_t\\} $.\n",
    "\n",
    "Inventory to be stored till next period is given by $ k_t = \\max\\{x_t-d_t,0\\} + q_t = x_{t+1} $.\n",
    "\n",
    "The profit in period $ t $ is given by\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\pi_t & = & p \\cdot \\text{sales}_t - r \\cdot x_{t+1} - c \\cdot (\\text{order made in period }t) \\\\\n",
    "& = & s_t p - k_t r - c \\mathbb{1}\\{q_t>0\\}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assuming all $ q_t \\ge 0 $, let $ \\sigma =  \\{q_t\\}_{t=1,\\dots,T} $ denote a feasible inventory policy.\n",
    "\n",
    "If $ d_t $ is stochastic the policy becomes a function of the period $ t $ inventory $ x_t $.\n",
    "\n",
    "The expected profit maximizing problem is given by\n",
    "\n",
    "$$\n",
    "{\\max}_{\\sigma} \\mathbb{E}\\Big[ \\sum_{t=0}^{T} \\beta^t \\pi_t \\Big],\n",
    "$$\n",
    "\n",
    "where $ \\beta $ is discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bellman equation for the problem\n",
    "\n",
    "Decisions: $ q_t $, how much new inventory to order\n",
    "\n",
    "What is important for the inventory decision at time period $ t $?\n",
    "- instanteneous utility (profit) contains $ x_t $ and $ d_t $\n",
    "- timing: (beginning of period) - current inventory - demand - (choice) order - stored inventory - (end of period)\n",
    "\n",
    "So, both $ x_t $ and $ d_t $ are taken into account for the new order to be made, forming the state space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "V(x_t,d_t) &=& \\max_{q_t \\ge 0} \\Big\\{ \\pi_t + \\beta \\mathbb{E}\\Big[ V\\big(x_{t+1} , d_{t+1} \\big) \\Big| x_t,d_t,q_t \\Big] \\Big\\} \\\\\n",
    "&=& \\max_{q_t \\ge 0} \\Big\\{ s_t p - k_t r - c \\mathbb{1}\\{q_t>0\\}\n",
    "+ \\beta \\mathbb{E}\\Big[ V\\big( k_t, d_{t+1} \\big) \\Big] \\Big\\}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "s_t &=& \\min\\{x_t,d_t\\} \\\\\n",
    "k_t &=& \\max\\{x_t-d_t,0\\} + q_t\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The expectation in the Bellman equation is taken over the distribution of the next period demand $ d_{t+1} $, which we assume is independent of any other variables and across time (idiosyncratic), thus the conditioning on $ (x_t,d_t,s_t) $ can be meaningfully dropped.\n",
    "\n",
    "Expectation can be written as an integral over the distribution of demand $ F(d) $, and since inventory is discrete it‚Äôs natural to assume demand is as well.\n",
    "\n",
    "The integral then transforms into a sum over the possible value of demand, weighted by their probabilities $ pr(d) $\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "V(x_t,d_t)\n",
    "&=& \\max_{q_t \\ge 0} \\Big\\{ s_t p - k_t r - c \\mathbb{1}\\{q_t>0\\}\n",
    "+ \\beta \\int V\\big( k_t, d \\big) \\partial F(d)  \\Big\\} \\\\\n",
    "&=& \\max_{q_t \\ge 0} \\Big\\{ s_t p - k_t r - c \\mathbb{1}\\{q_t>0\\}\n",
    "+ \\beta \\sum_d V\\big( k_t, d \\big) pr(d)  \\Big\\}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Today: deterministic case\n",
    "\n",
    "Let $ d $ be fixed and constant across time. How does the Bellman equation change?\n",
    "\n",
    "In the deterministic case with fixed $ d $, it can be simply dropped from the state space, and the Bellman equation can be simplified to\n",
    "\n",
    "$$\n",
    "\\begin{multline}\n",
    "V(x_t) = \\max_{q_t \\ge 0} \\big\\{ p \\min\\{x_t,d\\} - r \\big[ \\max\\{x_t-d,0\\} + q_t \\big] \\\\ - c \\mathbb{1}\\{q_t>0\\}\n",
    "+ \\beta V\\big( \\max\\{x_t-d,0\\} + q_t \\big) \\big\\}\n",
    "\\end{multline}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "class inventory_model:\n",
    "    '''Small class to hold model fundamentals and its solution'''\n",
    "\n",
    "    def __init__(self,label='noname',\n",
    "                 max_inventory=10,  # upper bound on the state space\n",
    "                 c = 3.2,            # fixed cost of order\n",
    "                 p = 2.5,           # profit per unit of good\n",
    "                 r = 0.5,           # storage cost per unit of good\n",
    "                 Œ≤ = 0.95,          # discount factor\n",
    "                 demand = 4         # fixed demand\n",
    "                 ):\n",
    "        '''Create model with default parameters'''\n",
    "        self.label=label # label for the model instance\n",
    "        self.c, self.p, self.r, self.Œ≤ = c, p, r, Œ≤\n",
    "        self.demand = demand\n",
    "        # created dependent attributes (it would be better to have them updated when underlying parameters change)\n",
    "        self.n = max_inventory+1    # number of inventory levels\n",
    "        self.upper = max_inventory  # upper boundary on inventory\n",
    "        self.x = np.arange(self.n)  # all possible values of inventory (state space)\n",
    "\n",
    "    def __repr__(self):\n",
    "        '''String representation of the model'''\n",
    "        return 'Inventory model labeled \"{}\"\\nParamters (c,p,r,Œ≤) = ({},{},{},{})\\nDemand={}\\nUpper bound on inventory {}' \\\n",
    "               .format (self.label,self.c,self.p,self.r,self.Œ≤,self.demand,self.upper)\n",
    "\n",
    "    def sales(self,x,d):\n",
    "        '''Sales in given period'''\n",
    "        return np.minimum(x,d)\n",
    "\n",
    "    def next_x(self,x,d,q):\n",
    "        '''Inventory to be stored, becomes next period state'''\n",
    "        return x - self.sales(x,d) + q\n",
    "\n",
    "    def profit(self,x,d,q):\n",
    "        '''Profit in given period'''\n",
    "        return self.p * self.sales(x,d) - self.r * self.next_x(x,d,q) - self.c * (q>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model=inventory_model(label='test')\n",
    "print(model)\n",
    "\n",
    "q=np.zeros(model.n)\n",
    "print('Current profits with zero orders\\n',model.profit(model.x,model.demand,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# illustration of broadcasting in the inventory model\n",
    "q=model.x[:,np.newaxis]  # column vector\n",
    "print('Current inventory\\n',model.x)\n",
    "print('Current sales\\n',model.sales(model.x,model.demand))\n",
    "print('Current orders\\n',q)\n",
    "print('Next period inventory\\n',model.next_x(model.x,model.demand,q))\n",
    "print('Current profits\\n',model.profit(model.x,model.demand,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def bellman(m,v0):\n",
    "    '''Bellman equation for inventory model\n",
    "       Inputs: model object\n",
    "               next period value function\n",
    "    '''\n",
    "    # create the grid of choices (same as x), column-vector\n",
    "    q = m.x[:,np.newaxis]\n",
    "    # compute current period profit (relying on numpy broadcasting to get the matrix with choices in rows)\n",
    "    p = m.profit(m.x,m.demand,q)\n",
    "    # indexes for next period value with extrapolation using last value\n",
    "    i = np.minimum(m.next_x(m.x,m.demand,q),m.upper)\n",
    "    # compute the Bellman maximand\n",
    "    vm = p + m.Œ≤*v0[i]\n",
    "    # find max and argmax\n",
    "    v1 = np.amax(vm,axis=0)   # maximum in every column\n",
    "    q1 = np.argmax(vm,axis=0) # arg-maximum in every column = order volume\n",
    "    return v1, q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "v = np.zeros(model.n)\n",
    "for i in range(3):\n",
    "    v,q = bellman(model,v)\n",
    "    print('Value =',v,'Policy =',q,sep='\\n',end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backwards induction algorithm\n",
    "\n",
    "Solver for the finite horizon dynamic programming problems\n",
    "\n",
    "1. Start at $ t=T $  \n",
    "1. Solve Bellman equation at $ t $, record optimal choice  \n",
    "1. Decrease $ t $ unless $ t=1 $, and return to previous step.  \n",
    "\n",
    "\n",
    "As result, for all $ t=1,\\dots,T $ have found the optimal choice (as a function of state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def solver_backwards_induction(m,T=10,verbose=False):\n",
    "    '''Backwards induction solver for the finite horizon case'''\n",
    "    # solution is time dependent\n",
    "    m.value  = np.zeros((m.n,T))\n",
    "    m.policy = np.zeros((m.n,T))\n",
    "    # main DP loop (from T to 1)\n",
    "    for t in range(T,0,-1):\n",
    "        if verbose:\n",
    "            print('Time period %d\\n'%t)\n",
    "        j = t-1 # index of value and policy functions for period t\n",
    "        if t==T:\n",
    "            # terminal period: ordering zero is optimal\n",
    "            m.value[:,j] = m.profit(m.x,m.demand,np.zeros(m.n))\n",
    "            m.policy[:,j] = np.zeros(m.n)\n",
    "        else:\n",
    "            # all other periods\n",
    "            m.value[:,j], m.policy[:,j] = bellman(m,m.value[:,j+1]) # next period to Bellman\n",
    "        if verbose:\n",
    "            print(m.value,'\\n')\n",
    "    # return model with updated value and policy functions\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = inventory_model(label='illustration')\n",
    "model=solver_backwards_induction(model,T=5,verbose=True)\n",
    "print('Optimal policy:\\n',model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_solution(model):\n",
    "    plt.step(model.x,model.value)\n",
    "    plt.legend([f'{i+1}' for i in range(model.value.shape[1])])\n",
    "    plt.title('Value function')\n",
    "    plt.show()\n",
    "    plt.step(model.x,model.policy)\n",
    "    plt.legend([f'{i+1}' for i in range(model.policy.shape[1])])\n",
    "    plt.title('Policy function (optimal order sizes)')\n",
    "    plt.show()\n",
    "\n",
    "plot_solution(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mod = inventory_model(label='production',max_inventory=50)\n",
    "mod.demand = 15\n",
    "mod.c = 5\n",
    "mod.p = 2.5\n",
    "mod.r = 1.4\n",
    "mod.Œ≤ = 0.975\n",
    "mod = solver_backwards_induction(mod,T=15)\n",
    "plot_solution(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Next steps\n",
    "\n",
    "- Stochastic demand  \n",
    "- Infinite horizon  \n",
    "\n",
    "\n",
    "More appropriate setup for the dynamic programming solution, so today only a simple special case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further learning resources\n",
    "\n",
    "- üìñ Adda and Russell Cooper ‚ÄúDynamic Economics. Quantitative Methods and Applications.‚Äù *Chapters: 2, 3.3*  \n",
    "- Bellman equation [https://en.wikipedia.org/wiki/Bellman_equation](https://en.wikipedia.org/wiki/Bellman_equation)  \n",
    "- Computer science view on DP [https://www.techiedelight.com/introduction-dynamic-programming](https://www.techiedelight.com/introduction-dynamic-programming)  \n",
    "- Popular optimal stopping [https://www.americanscientist.org/article/knowing-when-to-stop](https://www.americanscientist.org/article/knowing-when-to-stop)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1627474986.4333491,
  "filename": "27_dp_intro.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Foundations of Computational Economics #27"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}