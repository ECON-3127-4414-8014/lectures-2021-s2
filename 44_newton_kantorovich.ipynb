{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Foundations of Computational Economics #44\n",
    "\n",
    "by Fedor Iskhakov, ANU\n",
    "\n",
    "<img src=\"_static/img/dag3logo.png\" style=\"width:256px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Newton-Kantorovich method\n",
    "\n",
    "<img src=\"_static/img/lecture.png\" style=\"width:64px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"_static/img/youtube.png\" style=\"width:65px;\">\n",
    "\n",
    "[https://youtu.be/mRLxOWxqJbU](https://youtu.be/mRLxOWxqJbU)\n",
    "\n",
    "Description: Solving Bellman equation using Newton-Kantorovich iterations. Convergence rates. Polyalgorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have seen that different solution methods can differ drastically in their numerical performance:\n",
    "\n",
    "- VFI is simple to implement, global convergence, but the rate depends on the modulus of contraction  \n",
    "- Time iterations though theoretically equivalent, converge faster  \n",
    "- Policy iterations can converge much much faster  \n",
    "\n",
    "\n",
    "Newton-Kantorovich iterations is one more solution method: it adopts Newton-Raphson algorithm\n",
    "to find the fixed point on Bellman operator, and is therefore very fast as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leonid Vitalievich Kantorovich, 1912-1986\n",
    "\n",
    "<img src=\"_static/img/kantorovich.jpg\" style=\"height:200px;\">\n",
    "\n",
    "- Russian mathematician and economist, Leningrad/St.Petersburg, Novosibirsk  \n",
    "- Linear programming (see video 18) together with George Danzig  \n",
    "- Functional analysis: theoretical and numerical results  \n",
    "- 1975 Nobel prize for contributions to the theory of optimum allocation of resources, shared with Tjalling Koopmans  \n",
    "\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Leonid_Kantorovich](https://en.wikipedia.org/wiki/Leonid_Kantorovich)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kantorovich contribution\n",
    "\n",
    "üìñ L. V. Kantorovich 1948 ‚ÄúFunctional analysis and applied mathematics‚Äù, Uspekhi Mat. Nauk\n",
    "\n",
    "- application of functional analysis in numerical applications (in multiple works from 1937)  \n",
    "- built general approximation theory for solving functional equations  \n",
    "- generalized gradient descend and Newton methods for functional equations  \n",
    "- results on existence of solution to the approximated equation, convergence of approximated solutions to the true one, rates and error bounds of this approach  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kantorovich theorem\n",
    "\n",
    "- operator $ F: X \\rightarrow X $ maps Banach space $ X $ to itself  \n",
    "- to solve functional equation  \n",
    "\n",
    "\n",
    "$$\n",
    "F(x) = 0\n",
    "$$\n",
    "\n",
    "- form the sequence of approximate solutions $ x_0, x_1, \\dots $ such that  \n",
    "\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - F'(x_k)^{-1} F(x_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kantorovich theorem\n",
    "\n",
    "Let $ F $ be twice continuously differentiable on a ball $ B = \\{x: ||x-x_0|| \\le r\\} $, linear operator $ F'(x_0) $ is invertible,\n",
    "$ ||F'(x_0)^{-1}F(x_0)|| \\le \\eta $, $ ||F'(x_0)^{-1}F''(x)|| \\le K, \\, x\\in B $, and\n",
    "\n",
    "$$\n",
    "h = K \\eta < \\tfrac{1}{2}, \\;\\; r \\ge \\frac{1-\\sqrt{1-2h}}{h} \\eta.\n",
    "$$\n",
    "\n",
    "Then the equation $ F(x)=0 $ has a solution $ x^\\star \\in B : F(x^\\star)=0 $, and the sequence given by the Newton step converges to\n",
    "$ x^\\star $ with the quadratic rate\n",
    "\n",
    "$$\n",
    "||x_k - x^\\star|| \\le \\frac{\\eta}{h 2^k}(2h)^{2^k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NK iterations in dynamic modeling\n",
    "\n",
    "- solution of a dynamic model *in infinite horizon* is given by the fixed point of the Bellman operator  \n",
    "\n",
    "\n",
    "$$\n",
    "T(V)(x) = \\max_{a} \\big[ U(x,a) + \\beta \\mathbb{E}\\big\\{ V(x') \\big| x,a \\big\\} \\big]\n",
    "$$\n",
    "\n",
    "- we can apply Newton-Kantorovich method to solve the equation  \n",
    "\n",
    "\n",
    "$$\n",
    "T(V)(x) - V(x) = 0\n",
    "$$\n",
    "\n",
    "- **quadratic convergence** as compared to the linear convergence of successive approximating VFI solver  \n",
    "- requires to code the *(Fr√©chet) derivative of Bellman operator* (derivative defined on Banach spaces)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Rust model of bus engine replacement\n",
    "\n",
    "Application: Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zurcher (üìñ John Rust 1987, Econometrica)\n",
    "\n",
    "- Recall: video 28 for the setup, video 29 for implementation  \n",
    "- Choice to *keep* or *replace* the engine on the bus, conditional on the observed mileage and unobserved (by econometrician) other information  \n",
    "- Infinite horizon, discrete time  \n",
    "- Mileage process is discretized, band diagonal transition probability matrix estimated from the data directly  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transition matrix for mileage\n",
    "\n",
    "- If not replacing ($ d=0) $  \n",
    "\n",
    "\n",
    "$$\n",
    "\\Pi_{n \\times n} =\n",
    "\\begin{pmatrix}\n",
    "\\theta_{20} & \\theta_{21} & \\theta_{22} & 0 & \\cdot & \\cdot & \\cdot & 0 \\\\\n",
    "0 & \\theta_{20} & \\theta_{21} & \\theta_{22} & 0 & \\cdot & \\cdot & 0 \\\\\n",
    "0 & 0 &\\theta_{20} & \\theta_{21} & \\theta_{22} & 0 & \\cdot & 0 \\\\\n",
    "\\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\\n",
    "0 & \\cdot & \\cdot & 0 & \\theta_{20} & \\theta_{21} & \\theta_{22} & 0 \\\\\n",
    "0 & \\cdot & \\cdot & \\cdot & 0 & \\theta_{20} & \\theta_{21} & \\theta_{22} \\\\\n",
    "0 & \\cdot & \\cdot & \\cdot & \\cdot  & 0 & \\theta_{20} & 1-\\theta_{20} \\\\\n",
    "0 & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot  & 0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- If replacing ($ d=1 $), transition probabilities are given by the first row  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Zurcher‚Äôs preferences\n",
    "\n",
    "Instantaneous payoffs are given by the cost function that depends on the choice\n",
    "\n",
    "$$\n",
    "u(x_{t},d_t,\\theta_1)=\\left \\{\n",
    "\\begin{array}{ll}\n",
    "    -RC-c(0,\\theta_1) & \\text{if }d_{t}=\\text{replace}=1 \\\\\n",
    "    -c(x_{t},\\theta_1) & \\text{if }d_{t}=\\text{keep}=0\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "- $ RC $ = replacement cost  \n",
    "- $ c(x,\\theta_1) $ = cost of maintenance with preference parameters $ \\theta_1 $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Three independence assumptions for the error term\n",
    "\n",
    "1. Error terms are **independent across observations** due to random sampling  \n",
    "1. Error terms come in pairs, one for each decision $ d=0 $ and $ d=1 $, and are **independent across choices**  \n",
    "1. Conditional on mileage $ x $, there is no serial correlation in error terms **across time**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bellman equation\n",
    "\n",
    "$$\n",
    "V(x,\\varepsilon) = \\max_{d\\in \\{0,1\\}} \\big\\{ u(x,\\varepsilon_d,d) + \\beta \\mathbb{E}\\big[ V(x',\\varepsilon')\\big|x,\\varepsilon,d\\big] \\big\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V(x,\\varepsilon) = \\max_{d\\in \\{0,1\\}} \\big\\{ u(x,\\varepsilon_d,d) + \\beta\n",
    "\\int_{X} \\int_{\\Omega} V(x',\\varepsilon') p(x',\\varepsilon'|x,\\varepsilon,d) dx'd\\varepsilon' \\big\\}\n",
    "$$\n",
    "\n",
    "where $ \\varepsilon_d $ is the component of vector $ \\varepsilon \\in \\mathbb{R}^2 $ which corresponds to $ d $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rust assumptions\n",
    "\n",
    "**(AS)** Additive separability in preferences\n",
    "\n",
    "$$\n",
    "u(x,\\varepsilon_d,d) = u(x,d) + \\varepsilon_d,\n",
    "$$\n",
    "\n",
    "**(CI)** Conditional independence\n",
    "\n",
    "$$\n",
    "p(x',\\varepsilon'|x,\\varepsilon,d) = q(\\varepsilon'|x')\\cdot \\pi(x'|x,d)\n",
    "$$\n",
    "\n",
    "**(EV)** Extreme value Type I (EV1) distribution of $ \\varepsilon $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What Rust assumptions allow:\n",
    "\n",
    "$$\n",
    "V(x,\\varepsilon) = \\max_{d\\in \\{0,1\\}} \\big\\{ u(x,d) + \\varepsilon_d + \\beta\n",
    "\\int_{X} \\int_{\\Omega} V(x',\\varepsilon') \\pi(x'|x,d) q(\\varepsilon'|x') dx' d\\varepsilon' \\big\\}\n",
    "$$\n",
    "\n",
    "1. Separate out the deterministic part of **choice specific value function** $ v(x,d) $ (SA)  \n",
    "1. Compute the expectation by part (CI)  \n",
    "1. Use max-stability of EV1 to compute expectation w.r.t. $ \\varepsilon' $ (EV)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "V(x,\\varepsilon) = \\max_{d\\in \\{0,1\\}} \\big\\{ \\underbrace{u(x,d) + \\beta\n",
    "\\int_{X} \\Big( \\int_{\\Omega} V(x',\\varepsilon') q(\\varepsilon'|x') d\\varepsilon'\\Big)\n",
    "\\pi(x'|x,d) dx'}_{v(x,d)}\n",
    "+ \\varepsilon_d \\big\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V(x',\\varepsilon') = \\max_{d\\in \\{0,1\\}} \\big\\{ v(x',d) + \\varepsilon'_d \\big\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\big[ V(x',\\varepsilon')\\big|x,d\\big] =\n",
    "\\int_{X} \\log \\big( \\exp[v(x',0)] + \\exp[v(x',1)] \\big) \\pi(x'|x,d) dx'\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expected value function\n",
    "\n",
    "Let $ \\mathbb{E}\\big[ V(x',\\varepsilon')\\big|x,d\\big] = EV(x,d) $, then\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "EV(x,d) &=& \\int_{X} \\log \\big( \\exp[v(x',0)] + \\exp[v(x',1)] \\big) \\pi(x'|x,d) dx' \\\\\n",
    "v(x,d) &=& u(x,d) + \\beta EV(x,d)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "- this is Bellman equation *in expected value function space*  \n",
    "- when the state space is discrete the integral is, of course, a simple sum over future values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bellman equation and Bellman operator in expected value function space\n",
    "\n",
    "$$\n",
    "EV(x,d) = \\sum_{X} \\log \\big( \\exp[u(x',0) + \\beta EV(x',0)] + \\exp[u(x',1) + \\beta EV(x',1)] \\big) \\pi(x'|x,d)\n",
    "$$\n",
    "\n",
    "$$\n",
    "T^*(EV)(x,d) \\equiv \\sum_{X} \\log \\big( \\exp[u(x',0) + \\beta EV(x',0)] + \\exp[u(x',1) + \\beta EV(x',1)] \\big) \\pi(x'|x,d)\n",
    "$$\n",
    "\n",
    "Solution to the Bellman functional equation $ EV(x,d) $ is also a fixed point of $ T^* $ operator, $ T^*(EV)(x,d)=EV(x,d) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choice probabilities\n",
    "\n",
    "Once the fixed point is found, the policy function is given by the *optimal* choice probability $ P(d|x) $ which has the *logit* structure due to assumption EV:\n",
    "\n",
    "$$\n",
    "P(0|x) = \\frac{\\exp[ u(x,0) + \\beta EV(x,0)  ]}{\\sum_{d\\in \\{0,1\\}} \\exp[u(x,d) + \\beta EV(x,d)]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(0|x) = \\frac{1}{1 + \\exp[u(x,1)  - u(x,0) + \\beta EV(x,1) - \\beta EV(x,0)]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Possible solution methods for Rust‚Äôs model\n",
    "\n",
    "- infinite horizon  \n",
    "- discretized mileage which is the only state (in EV formulation) = finite state space  \n",
    "- discrete choice  \n",
    "- idiosyncratic random components  \n",
    "\n",
    "\n",
    "(see video 37 on DP theory)\n",
    "\n",
    "1. value function iterations (VFI)  \n",
    "1. policy iterations (see video 43)  \n",
    "1. Newton-Kantorovich method (NK iterations)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NK iterations method\n",
    "\n",
    "$$\n",
    "EV(x,d) = T^*(EV)(x,d) = \\Gamma(EV)(x,d) \\quad\\Leftrightarrow\\quad (I - \\Gamma)(EV)(x,d)=\\mathbb{0}\n",
    "$$\n",
    "\n",
    "The **NK iteration** is\n",
    "\n",
    "$$\n",
    "EV_{k+1} = EV_{k} - (I-\\Gamma')^{-1} (I-\\Gamma)(EV_k)\n",
    "$$\n",
    "\n",
    "- The new operator is the difference between the identity operator \\$I\\$ and Bellman operator $ \\Gamma = T^* $  \n",
    "- $ \\mathbb{0} $ is zero function  \n",
    "- $ I-\\Gamma' $ is a Fr√©chet derivative of the operator $ I-\\Gamma $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finite approximations\n",
    "\n",
    "- let $ n $ denote the number of state points (in mileage)  \n",
    "- $ EV(x,d) $ is given by a vector of length $ n $, assuming that the first element is reused to describe the expected value of replacing  \n",
    "- $ T^*(EV)(x,d) = \\Gamma(EV)(x,d) $ is a non-linear $ n $-valued multivariate function of $ EV $  \n",
    "- Fr√©chet derivative $ I-\\Gamma' $ is an $ n \\times n $ matrix of first order derivatives of each output of $ T^*(EV)(x,d) $ w.r.t. each input  \n",
    "\n",
    "\n",
    "*NK iterations on finite approximations are similar to solving a system of* $ n $ *equations with* $ n $ *unknowns with Newton method!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix expression for the finite approximation of the Bellman operator\n",
    "\n",
    "$$\n",
    "EV(x,d) = \\sum_{X} \\log \\big( \\exp[u(x',0) + \\beta EV(x',0)] + \\exp[u(x',1) + \\beta EV(x',1)] \\big) \\pi(x'|x,d)\n",
    "$$\n",
    "\n",
    "$$\n",
    "EV = \\Pi \\cdot L \\big( U(\\text{keep}) + \\beta EV, U(\\text{replace}) + \\beta EV[0] \\big)\n",
    "$$\n",
    "\n",
    "- $ EV $ is a $ n \\times 1 $ column vector  \n",
    "- $ \\Pi $ is the $ n \\times n $ matrix of mileage transition probabilities  \n",
    "- $ U(\\cdot) $ is a column-vector of costs for all points in the state space, conditional on decision  \n",
    "- $ L(\\cdot,\\cdot) $ is the logsum function returning an $ n \\times 1 $ vector  \n",
    "- notation $ \\bullet[i] $ denotes the $ i $-th element a vector  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementation of Fr√©chet derivative\n",
    "\n",
    "Finite approximation of the Bellman operator is\n",
    "\n",
    "$$\n",
    "\\Gamma(EV) = \\Pi \\cdot L \\big( U(\\text{keep}) + \\beta EV, U(\\text{replace}) + \\beta EV[0] \\big)\n",
    "$$\n",
    "\n",
    "Fr√©chet derivative w.r.t. $ EV $ is then given by $ n \\times n $ matrix\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\Gamma}{\\partial EV} = \\Pi \\cdot \\frac{\\partial L\\big( U(\\text{keep}) + \\beta EV, U(\\text{replace}) + \\beta EV[0] \\big)}{\\partial EV}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differentiating the logsum function w.r.t. a scalar\n",
    "\n",
    "- the logsum function $ L(w_1,w_2) = \\log\\big[ \\exp(w_1) + \\exp(w_2)  \\big] $  \n",
    "- ($ L(w_1,w_2) $ is the expectation of maximum of $ w_i + \\varepsilon_i $, where\n",
    "  $ \\varepsilon_1, \\varepsilon_2 $ are distributed independently with type 1 extreme value distribution)  \n",
    "- let $ p_i = {\\exp(w_i) \\over \\exp(w_1) + \\exp(w_2)} $ denote the corresponding *choice probabilities*  \n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w_1,w_2)}{\\partial x} = p_1 \\frac{\\partial w_1}{\\partial x} + p_2 \\frac{\\partial w_2}{\\partial x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differentiating the logsum function w.r.t. $ EV[i],\\;i>0 $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial EV} = \\beta\n",
    "\\begin{pmatrix}\n",
    "\\bullet & 0 & 0 & 0 & \\cdot & 0 \\\\\n",
    "\\bullet & P[1] & 0 & 0 & \\cdot & 0 \\\\\n",
    "\\bullet & 0 & P[2] & 0 & \\cdot & 0 \\\\\n",
    "\\bullet & 0 & 0 & P[3] & \\cdot & 0 \\\\\n",
    "\\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\\n",
    "\\bullet & 0 & 0 & 0 & \\cdot & P[n-1]\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $ P[i] $ is a shortcut notation for *probability of keeping* $ P(0|x[i]) $ at state point $ i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differentiating the logsum function w.r.t. $ EV[0] $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial EV[0]} = \\beta\n",
    "\\begin{pmatrix}\n",
    "P[0] + \\bar{P}[0] \\\\\n",
    "\\bar{P}[1] \\\\\n",
    "\\bar{P}[2] \\\\\n",
    "\\cdot \\\\\n",
    "\\bar{P}[n-1]\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $ \\bar{P}[i] $ is a shortcut notation for *probability of replacing* $ P(1|x[i]) $ at state point $ i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix notation for the Fr√©chet derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\Gamma}{\\partial EV} = \\beta\n",
    "\\begin{pmatrix}\n",
    "\\theta_{20} & \\theta_{21} & \\theta_{22} & 0 & \\cdot & \\cdot & \\cdot & 0 \\\\\n",
    "0 & \\theta_{20} & \\theta_{21} & \\theta_{22} & 0 & \\cdot & \\cdot & 0 \\\\\n",
    "\\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\\n",
    "0 & \\cdot & \\cdot & \\cdot & 0 & \\theta_{20} & \\theta_{21} & \\theta_{22} \\\\\n",
    "0 & \\cdot & \\cdot & \\cdot & \\cdot  & 0 & \\theta_{20} & 1-\\theta_{20} \\\\\n",
    "0 & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot  & 0 & 1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "P[0] + \\bar{P}[0] & 0 & 0 & 0 & \\cdot & 0 \\\\\n",
    "\\bar{P}[1] & P[1] & 0 & 0 & \\cdot & 0 \\\\\n",
    "\\bar{P}[2] & 0 & P[2] & 0 & \\cdot & 0 \\\\\n",
    "\\bar{P}[3] & 0 & 0 & P[3] & \\cdot & 0 \\\\\n",
    "\\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\\n",
    "\\bar{P}[n-1] & 0 & 0 & 0 & \\cdot & P[n-1]\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix notation for the Fr√©chet derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\Gamma}{\\partial EV} = \\beta\n",
    "\\begin{pmatrix}\n",
    "\\theta_{20} P[0] & \\theta_{21} P[1] & \\theta_{22} P[2] & 0 & \\cdot & \\cdot & \\cdot & 0 \\\\\n",
    "0 & \\theta_{20}P[1] & \\theta_{21}P[2] & \\theta_{22}P[3] & 0 & \\cdot & \\cdot & 0 \\\\\n",
    "0 & 0 &\\theta_{20}P[2] & \\theta_{21}P[3] & \\theta_{22}P[4] & 0 & \\cdot & 0 \\\\\n",
    "\\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\\n",
    "\\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot  & 0 & P[n-1]\n",
    "\\end{pmatrix}\n",
    "+ \\beta\n",
    "\\begin{pmatrix}\n",
    "\\Pi \\bar{P}, 0, \\dots, 0\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NK iterations algorithm\n",
    "\n",
    "1. Initialize value function at $ EV_0 $ (starting values matter!)  \n",
    "1. Perform the Newton-Kantorovich step, computing the policy function along the way of applying the Bellman operator $ \\Gamma(\\cdot) $  \n",
    "\n",
    "\n",
    "$$\n",
    "EV_{k+1} = EV_{k} - (I-\\Gamma')^{-1} (I-\\Gamma)(EV_k)\n",
    "$$\n",
    "\n",
    "1. Repeat until convergence value function space  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class zurcher():\n",
    "    '''Harold Zurcher bus engine replacement model class, VFI version'''\n",
    "\n",
    "    def __init__(self,\n",
    "                 n = 175,           # number of state points\n",
    "                 RC = 11.7257,      # replacement cost\n",
    "                 c = 2.45569,       # parameter of maintance cost (theta_1)\n",
    "                 p = [0.0937,0.4475,0.4459,0.0127],  # probabilities of transitions (theta_2)\n",
    "                 beta = 0.9999):    # discount factor\n",
    "        '''Init for the Zurcher model object'''\n",
    "        assert sum(p)<=1.0, 'Transition probability parameters must sum up to <1'\n",
    "        self.RC, self.c, self.p, self.beta, self.n= RC, c, p, beta, n\n",
    "\n",
    "    @property\n",
    "    def n(self):\n",
    "        '''Attrinute getter for n'''\n",
    "        return self.__n\n",
    "\n",
    "    @n.setter\n",
    "    def n(self, value):\n",
    "        '''Attribute n setter'''\n",
    "        self.__n = value\n",
    "        self.grid = np.arange(self.__n)\n",
    "        self.trpr = self.__transition_probs()\n",
    "\n",
    "    def __repr__(self):\n",
    "        '''String representation of the Zurcher model'''\n",
    "        return 'Rust model of bus engine replacement (id={})'.format(id(self))\n",
    "\n",
    "    def __transition_probs(self):\n",
    "        '''Computing the transision probability matrix'''\n",
    "        trpr = np.zeros((self.__n,self.__n))  # init\n",
    "        probs = self.p + [1-sum(self.p)]  # ensure sum up to 1\n",
    "        for i,p in enumerate(probs):\n",
    "            trpr += np.diag([p]*(self.__n-i),k=i)\n",
    "        trpr[:,-1] = 1.-np.sum(trpr[:,:-1],axis=1)\n",
    "        return trpr\n",
    "\n",
    "    def bellman(self,ev0,deriv=False):\n",
    "        '''Bellman operator for the model\n",
    "           Depending on deriv argument, returns 2 or 3 outputs (Fr√©chet derivative)\n",
    "        '''\n",
    "        x = self.grid  # points in the next period state\n",
    "        mcost = -0.001*x*self.c                         # 1-dim array of maintenance costs\n",
    "        vx0 = mcost + self.beta * ev0                   # 1-dim array v(x,0), keep\n",
    "        vx1 = mcost[0] - self.RC + self.beta * ev0[0]   # 1-dim array v(x,1), replace\n",
    "        M = np.maximum(vx0,vx1)                         # de-max values to avoid exp(large number)\n",
    "        logsum = M + np.log(np.exp(vx0-M) + np.exp(vx1-M))\n",
    "        ev1 = self.trpr @ logsum                        # 1-dim array after matrix multiplication\n",
    "        pk = 1/( np.exp(vx1-vx0)+1 )                    # choice prob to keep\n",
    "        if not deriv:\n",
    "            return ev1, pk\n",
    "        # Fr√©chet derivative\n",
    "        dev1 = self.beta * self.trpr * pk[np.newaxis,:] # element-wise, pk in rows\n",
    "        dev1[:,0] += self.beta * self.trpr @ (1-pk)     # w.r.t. EV[0] special case\n",
    "        return ev1, pk, dev1\n",
    "\n",
    "    def solve_vfi(self,tol=1e-6,maxiter=100,callback=None):\n",
    "        '''Solves the Rust model using value function iterations\n",
    "        '''\n",
    "        ev0 = np.zeros(self.n) # initial point for VFI\n",
    "        err0 = 1.0 # initial lagged error\n",
    "        for iter in range(maxiter):  # main loop\n",
    "            ev1, pk = self.bellman(ev0)  # update approximation\n",
    "            err = np.amax(np.abs(ev0-ev1))\n",
    "            if callback:\n",
    "                callback(iter=iter,model=self,ev1=ev1,ev0=ev0,err=err,err_prev=err0,pk=pk,method='vfi',itertype='sa')\n",
    "            if err<tol:\n",
    "                break  # break out if converged\n",
    "            ev0 = ev1  # get ready to the next iteration\n",
    "            err0 = err\n",
    "        else:\n",
    "            raise RuntimeError('Failed to converge in %d iterations'%maxiter)\n",
    "        return ev1, pk\n",
    "\n",
    "    def solve_nk(self, maxiter=100, tol=1e-6, callback=None):\n",
    "        '''Solves the model using the Newton-Kantorovich iterations\n",
    "        '''\n",
    "        ev0 = np.zeros(self.n) # initial point\n",
    "        err0 = 1.0 # initial lagged error\n",
    "        for iter in range(maxiter):\n",
    "            ev1,pk,dev = self.bellman(ev0,deriv=True) # compute with Fr√©chet derivative\n",
    "            ev1 = ev0 - np.linalg.solve(np.eye(self.n)-dev,ev0 - ev1)  # NK step\n",
    "            err = np.max(np.abs(ev1-ev0))\n",
    "            if callback:\n",
    "                callback(iter=iter,model=self,ev1=ev1,ev0=ev0,err=err,err_prev=err0,pk=pk,method='nk',itertype='nk')\n",
    "            if err < tol:\n",
    "                break  # break out if converged\n",
    "            ev0 = ev1  # get ready to the next iteration\n",
    "            err0 = err\n",
    "        else:\n",
    "            raise RuntimeError('Failed to converge in %d iterations'%maxiter)\n",
    "        ev1,pk = self.bellman(ev1) # compute choice probabilities after convergence\n",
    "        return ev1,pk\n",
    "\n",
    "    def solve_poly(self,\n",
    "                   maxiter=100,\n",
    "                   tol=1e-10,\n",
    "                   sa_min=5,         # minimum number of contraction steps\n",
    "                   sa_max=25,        # maximum number of contraction steps\n",
    "                   switch_tol=0.025, # tolerance of the switching rule\n",
    "                   callback=None):\n",
    "        '''Solves the model using the poly-algorithm'''\n",
    "        ev0 = np.zeros(self.n) # initial point\n",
    "        err0 = 1.0 # initial lagged error\n",
    "        nk = False # start with successive approximations\n",
    "        for iter in range(maxiter):\n",
    "            ev1,pk,dev = self.bellman(ev0,deriv=True) # update EV for both types of a step\n",
    "            err = np.max(np.abs(ev1-ev0))\n",
    "            nk = True if iter>= sa_max else nk  # have to switch to NK after sa_max\n",
    "            nk = nk or (iter>=sa_min and abs(err/err0 - self.beta)<switch_tol)  # check if need to switch to NK\n",
    "            if nk:\n",
    "                ev1 = ev0 - np.linalg.solve(np.eye(self.n)-dev,ev0 - ev1)  # NK step\n",
    "                err = np.max(np.abs(ev1-ev0))\n",
    "            if callback:\n",
    "                itertype = 'nk' if nk else 'sa'  # label for the iteration type\n",
    "                callback(iter=iter,model=self,ev1=ev1,ev0=ev0,err=err,err_prev=err0,pk=pk,method='poly',itertype=itertype)\n",
    "            if err < tol:\n",
    "                break  # break out if converged\n",
    "            ev0 = ev1  # get ready to the next iteration\n",
    "            err0 = err\n",
    "        else:\n",
    "            raise RuntimeError('No convergence: maximum number of iterations achieved! Increase maxiter')\n",
    "        ev1,pk = self.bellman(ev1) # compute choice probabilities after convergence\n",
    "        return ev1,pk\n",
    "\n",
    "    def solve_show(self,solver='vfi',verbosity=0,plot=True,**kvargs):\n",
    "        '''Illustrate solution for given solver = {vfi,nk,poly} and\n",
    "           print errors/relative errors from iterations (when verbose=True)\n",
    "           All other arguments are passed to the solver\n",
    "        '''\n",
    "        if solver=='vfi':\n",
    "            chosen_solver = self.solve_vfi\n",
    "        elif solver=='nk':\n",
    "            chosen_solver = self.solve_nk\n",
    "        elif solver=='poly':\n",
    "            chosen_solver = self.solve_poly\n",
    "        else:\n",
    "            raise RuntimeError('Unknown solver in solve_show()')\n",
    "        if plot:\n",
    "            fig1, (ax1,ax2) = plt.subplots(1,2,figsize=(14,8))\n",
    "            ax1.grid(b=True, which='both', color='0.65', linestyle='-')\n",
    "            ax2.grid(b=True, which='both', color='0.65', linestyle='-')\n",
    "            ax1.set_xlabel('Mileage grid')\n",
    "            ax2.set_xlabel('Mileage grid')\n",
    "            ax1.set_title(f'Value function ({solver})')\n",
    "            ax2.set_title(f'Probability of replacing the engine ({solver})')\n",
    "        def callback(**argvars):\n",
    "            iter,itertype,err,derr = argvars['iter'],argvars['itertype'],argvars['err'],argvars['err_prev']\n",
    "            mod, ev, pk = argvars['model'],argvars['ev1'],argvars['pk']\n",
    "            if verbosity>1:\n",
    "                if iter==0:\n",
    "                    print('Solver = %s'%solver)\n",
    "                    print('-'*42)\n",
    "                    print('%7s %16s %16s'%('iter','err','err(i)/err(i-1)'))\n",
    "                    print('-'*42)\n",
    "                print('%4d %2s %16.4e %16.12f'%(iter,itertype[:2],err,err/derr))\n",
    "            elif verbosity>0:\n",
    "                if iter==0:\n",
    "                    print('Solver = %s'%solver)\n",
    "                    print('-'*22)\n",
    "                    print('%4s %16s'%('iter','err'))\n",
    "                    print('-'*22)\n",
    "                print('%4d %16.4e'%(iter,err))\n",
    "            if plot:\n",
    "                ax1.plot(mod.grid,ev,color='k',alpha=0.25)\n",
    "                ax2.plot(mod.grid,pk,color='k',alpha=0.25)\n",
    "            callback.nriter = iter  # save iter in function object attribute\n",
    "        # run the chosen solver\n",
    "        ev,pk = chosen_solver(callback=callback,**kvargs)\n",
    "        if plot:\n",
    "            # add solutions\n",
    "            ax1.plot(self.grid,ev,color='r',linewidth=2.5)\n",
    "            ax2.plot(self.grid,pk,color='r',linewidth=2.5)\n",
    "            plt.show()\n",
    "        print('{} solved with {} in {} iterations'.format(self,solver,callback.nriter))\n",
    "        return ev,pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# compare SA, NK\n",
    "model = zurcher(beta=0.9)  # try different value of beta\n",
    "ev1,pk1 = model.solve_show(maxiter=1500)\n",
    "ev2,pk2 = model.solve_show(solver='nk')\n",
    "print()\n",
    "print('Max diff between value functions is ' ,np.amax(np.abs(ev1-ev2)))\n",
    "print('Max diff between policy functions is',np.amax(np.abs(pk1-pk2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions to think about\n",
    "\n",
    "- Does VFI algorithm always converge?  \n",
    "- What determines the speed of convergence of the VFI algorithm?  \n",
    "- Does NK algorithm always converge?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of VFI vs Newton-Kantorovich solution methods\n",
    "\n",
    "- VFI is **globally convergent** (Bellman is contraction mappint $ \\Rightarrow $ single fixed point)  \n",
    "- VFI convergent rate is $ beta $, **very slow** in approaching the fixed point when \\$beta\\$ is close to one  \n",
    "\n",
    "\n",
    "vs.\n",
    "\n",
    "- Newton-Kantorovich has **quadratic convergence** rate  \n",
    "- Newton-Kantorovich is **sensitive to starting point**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# compare SA, NK\n",
    "model = zurcher(beta=0.975)\n",
    "ev1,pk1 = model.solve_show(maxiter=1500,verbosity=1,plot=False)\n",
    "ev2,pk2 = model.solve_show(solver='nk',verbosity=1,plot=False)\n",
    "print()\n",
    "print('Max diff between value functions is ' ,np.amax(np.abs(ev1-ev2)))\n",
    "print('Max diff between policy functions is',np.amax(np.abs(pk1-pk2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Poly-algorithm\n",
    "\n",
    "- NK method may not be convergent at the initial point  \n",
    "- Successive apprizimataion (SA) iterations, however, are always convergent  \n",
    "\n",
    "\n",
    "**Poly algorithm** is combination of SA and NK:\n",
    "\n",
    "1. Start with SA iterations  \n",
    "1. At approximately optimal time switch to NK iterations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When to switch to NK iterations?\n",
    "\n",
    "Suppose $ EV_{k-1} = {EV}^\\star + C $ (where $ {EV}^\\star $ is the fixed point)\n",
    "\n",
    "$$\n",
    "err_{k} = ||EV_{k-1}-EV_{k}|| = ||{EV}^\\star+C - T^*({EV}^\\star+C)|| = ||{EV}^\\star + C - {EV}^\\star - \\beta C|| = C (1-\\beta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "err_{k+1} = ||EV_{k}-EV_{k+1}|| = ||T^*({EV}^\\star+C) - T^*(T^*({EV}^\\star+C))|| = ||{EV}^\\star + \\beta C - {EV}^\\star - \\beta^2 C|| = \\beta C (1-\\beta)\n",
    "$$\n",
    "\n",
    "- Then the ration of two errors $ \\frac{err_{k+1}}{err_{k}} = \\beta $ when the current approximation is a constant away from the fixed point.  \n",
    "- NK iteration will immediately ‚Äústrip away‚Äù the constant  \n",
    "\n",
    "\n",
    "**Thus, switch to NK iteration when** $ \\frac{err_{k+1}}{err_{k}} $ **is close to** $ \\beta $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# when to switch from SA to NK\n",
    "model = zurcher(beta=0.975)\n",
    "model.solve_show(maxiter=1500,verbosity=2,plot=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# compare SA, NK and polyalgorithm\n",
    "m = zurcher(beta=0.975)\n",
    "ev,pk = m.solve_show(tol=1e-10,maxiter=1500)\n",
    "ev,pk = m.solve_show(tol=1e-10,solver='nk',plot=False)\n",
    "polyset = {'sa_min':10,\n",
    "           'sa_max':100,\n",
    "           'switch_tol':0.000215,\n",
    "          }\n",
    "ev,pk = m.solve_show(tol=1e-10,verbosity=2,solver='poly',**polyset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# original parameters from Rust 1987\n",
    "m = zurcher()\n",
    "polyset = {'sa_min':10,\n",
    "           'sa_max':100,\n",
    "           'switch_tol':0.0005,\n",
    "          }\n",
    "ev,pk = m.solve_show(tol=1e-10,solver='poly',**polyset)\n",
    "# time the solver\n",
    "%timeit -n 5 -r 10 m.solve_poly(tol=1e-10,**polyset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Further learning resources\n",
    "\n",
    "- üìñ John Rust (1987, Econometrica) ‚ÄúOptimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zurcher‚Äù  \n",
    "- details on Kantorovich theorem [https://en.wikipedia.org/wiki/Kantorovich_theorem](https://en.wikipedia.org/wiki/Kantorovich_theorem)  \n",
    "- NFXP manual with detailed instructions on Fr√©chet derivative in Rust model  \n",
    "- on convergence rate (order of convergence) [https://youtu.be/JTinepDn1dI](https://youtu.be/JTinepDn1dI)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1627474987.0866802,
  "filename": "44_newton_kantorovich.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Foundations of Computational Economics #44"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}